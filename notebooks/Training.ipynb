{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training <a id=\"model-training\"></a>\n",
    "\n",
    "This notebook is for training the language recognition models. These will be seuquence models that receive\n",
    "character-level tokenized sentences and output a label (language of the text). They will train using the dataset\n",
    "prepared with the `make_dataset` script from the TED talks data.\n",
    "\n",
    "## Contents\n",
    "- [Dataset Preparation](#dataset-preparation)\n",
    "- [Model Fitting](#model-fitting)\n",
    "- [Save Model](#save-model)\n",
    "- [Model Evaluation](#model-evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import date\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "dotenv.load_dotenv(os.path.join(\"..\", \".env\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation <a id=\"dataset-preparation\"></a>\n",
    "\n",
    "First we prepare the dataset for model training. We will make use of the keras preprocessing API and TensorFlow datasets.\n",
    "\n",
    "[Back to top](#model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and utilities\n",
    "tok = tf_text.UnicodeCharTokenizer()\n",
    "PAD_LENGTH: int = 128  # Length to which to pad sequences.\n",
    "BATCH_SIZE: int = 1024  # Size of dataset batches\n",
    "VOCAB_SIZE: int = 256  # Vocabulary size (we use ASCII characters)\n",
    "DATASET_PATH: str = os.path.join(\"..\", \"data\", \"processed-text\")  # Path to dataset files\n",
    "\n",
    "# Labels\n",
    "LANGUAGES = [\n",
    "    \"English\",\n",
    "    \"French\",\n",
    "    \"Italian\",\n",
    "    \"Portuguese\",\n",
    "    \"Spanish\",\n",
    "    \"Turkish\"\n",
    "]\n",
    "\n",
    "LANGUAGES.sort()\n",
    "NUM_CLASSES: int = len(LANGUAGES)  # Number of categories to classify\n",
    "\n",
    "# Functions to prepare input for model\n",
    "def drop_newlines(strings):\n",
    "    return tf.strings.regex_replace(strings, r\"\\n\", \"\")\n",
    "\n",
    "def prepare_input(strings):\n",
    "    \"\"\"\n",
    "    Prepare the strings for the model by dropping newlines and tokenizing them.\n",
    "    \"\"\"\n",
    "    # Remove newline characters from strings\n",
    "    strings_clean = drop_newlines(strings)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tok.tokenize(strings_clean)\n",
    "    \n",
    "    # Pad sequences\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Make a dataset for the given data split (train or test) by making a\n",
    "    TextLineDataset for each language and then interleaving them.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i, lang in enumerate(LANGUAGES):\n",
    "        path = os.path.join(DATASET_PATH, split, lang)\n",
    "        \n",
    "        # Get TextxLineDataset for this language\n",
    "        ds = tf.data.TextLineDataset([\n",
    "            os.path.join(path, file_name)\n",
    "            for file_name in os.listdir(path)\n",
    "        ])\n",
    "        \n",
    "        # Prepare input: tokenize, pad token sequences, add labels (int)\n",
    "        ds = ds.map(prepare_input)\\\n",
    "            .padded_batch(1, padded_shapes=PAD_LENGTH)\\\n",
    "            .unbatch()\\\n",
    "            .map(lambda tokens: (tokens, i))\n",
    "        datasets.append(ds)\n",
    "    \n",
    "    # Interleave datasets\n",
    "    merged = tf.data.Dataset.from_tensor_slices(datasets)\\\n",
    "        .interleave(lambda x: x)\\\n",
    "        .shuffle(1024)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "train_dataset = make_dataset(\"train\")\n",
    "valid_dataset = make_dataset(\"test\")\n",
    "\n",
    "# Preprocess data\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(128)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting <a id=\"model-fitting\"></a>\n",
    "\n",
    "Now we will build and fit the models to the data.\n",
    "\n",
    "[Back to top.](#model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(embedding_dim: int = 32, lstm_dim: int = 32, dropout_rate: float = 0.3) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Make the prediction model!\n",
    "    \"\"\"\n",
    "    assert 0.<= dropout_rate < 1.\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # Input tokens and embed\n",
    "    inputs = keras.layers.Input(shape=(PAD_LENGTH,), dtype=tf.int32)\n",
    "    embed = keras.layers.Embedding(\n",
    "        input_dim=VOCAB_SIZE, \n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True,\n",
    "        input_length=PAD_LENGTH)(inputs)\n",
    "    \n",
    "    # Recurring / Convolutional layers\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(\n",
    "        lstm_dim,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=True))(embed)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = keras.layers.Conv1D(\n",
    "        2 * lstm_dim,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\")(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Pooling, final output\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = keras.layers.Dense(16, activation=keras.layers.LeakyReLU())(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    \n",
    "    # Model\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(lstm_dim=16, dropout_rate=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT Params\n",
    "EPOCHS = 2\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.02),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=\"accuracy\"\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    verbose=VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model <a id=\"save-model\"></a>\n",
    "\n",
    "Now we save the model as an `h5` file and optionally as a `tflite` file to allow inference with a lighter version of TF.\n",
    "\n",
    "[Back to top.](#model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tflite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and upload to S3\n",
    "model_dir = os.path.join(\"..\", \"data\", \"models\")\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "model_file = os.path.join(model_dir, \"model.h5\")\n",
    "model.save(model_file)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\n",
    "    model_file,\n",
    "    os.getenv(\"S3_BUCKET\"),\n",
    "    f\"models/model-{date.today():%Y-%m-%d}.h5\"\n",
    ")\n",
    "\n",
    "if save_tflite:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    lite_model = converter.convert()\n",
    "    \n",
    "    # Save lite model in the server's directory\n",
    "    out_dir = os.path.join(\"..\", \"service\", \"src\", \"assets\")\n",
    "    if not os.path.isdir(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    \n",
    "    out_file = os.path.join(out_dir, \"model.tflite\")\n",
    "    with open(out_file, \"wb\") as f:\n",
    "        f.write(lite_model)\n",
    "        print(out_file)\n",
    "    \n",
    "    del converter, lite_model\n",
    "    s3.upload_file(\n",
    "        Bucket=os.getenv(\"S3_BUCKET\"),\n",
    "        Key=\"models/service/model.tflite\",\n",
    "        Filename=out_file\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation <a id=\"model-evaluation\"></a>\n",
    "\n",
    "Now we evaluate the model's performance.\n",
    "\n",
    "[Back to top.](#model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "test_sentences = [\n",
    "    \"Mary had a little lamb, its fleece was white as snow\",\n",
    "    \"nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\",\n",
    "    \"No tengo penas ni tengo amores y asi no sufro de sinsabores\",\n",
    "    \"Non seulement prend plaisir aux malheurs des autres, mais aussi neglige le bien-etre de sa famille\"\n",
    "]\n",
    "\n",
    "prepared = prepare_input(test_sentences)\n",
    "prepared = prepared.to_tensor(0, shape=(prepared.shape[0], PAD_LENGTH))\n",
    "pred = model.predict(prepared)\n",
    "langs = np.argmax(pred, axis=1).tolist()\n",
    "conf = np.max(pred, axis=1).tolist()\n",
    "\n",
    "for sentence, lang, confidence in zip(test_sentences, langs, conf):\n",
    "    print(\"Sentence:\")\n",
    "    print(sentence)\n",
    "    print()\n",
    "    \n",
    "    print(\"Predicted Language:\")\n",
    "    print(LANGUAGES[lang])\n",
    "    print(\"Confidence: %.2f\" % (100 * confidence))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
